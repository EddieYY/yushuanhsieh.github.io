---
layout: single
title:  "gRPC Client 與 Server 連線流程"
date:   2019-04-05 08:00:00 +0800
categories: WebDevelopment
---
## 前言
其實一開始的目的是想要研究 gRPC 的 retry 機制，不過在了解 retry 之前勢必要先說明整個 gRPC client 與 server 建立連線的過程，因此就先用 source code trace 的方式簡單說明在呼叫 `grpc.Dial` 後所執行的連線流程，包含 gRPC 實現 load-balancing 的機制。

## packages
grpc-go v1.19.1

## grpc.Dial 的背後
以下是 client 向 server 發送 request 的基本方式。

```go
conn, err := grpc.Dial(serverIpAddress, grpc.WithInsecure())
ctx := context.Background()
client := system.NewSysManagerClient(conn)
stream, err := client.GetInfo(ctx, &system.Reqs{})
```

透過 source code trace 來瞧瞧過程中發生什麼事情～

```go
// Notice: 部分 source code 被移除

func Dial(target string, opts ...DialOption) (*ClientConn, error) {
	return DialContext(context.Background(), target, opts...)
}

func DialContext(ctx context.Context, target string, opts ...DialOption) (conn *ClientConn, err error) {
  if cc.dopts.resolverBuilder == nil {
		// Only try to parse target when resolver builder is not already set.
		cc.parsedTarget = parseTarget(cc.target)
		grpclog.Infof("parsed scheme: %q", cc.parsedTarget.Scheme)
		cc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)
		if cc.dopts.resolverBuilder == nil {
			// If resolver builder is still nil, the parsed target's scheme is
			// not registered. Fallback to default resolver and set Endpoint to
			// the original target.
			grpclog.Infof("scheme %q not registered, fallback to default scheme", cc.parsedTarget.Scheme)
			cc.parsedTarget = resolver.Target{
				Scheme:   resolver.GetDefaultScheme(),
				Endpoint: target,
			}
			// Default use passthrough resolver builder (passthrough.go)
			cc.dopts.resolverBuilder = resolver.Get(cc.parsedTarget.Scheme)
		}
	}
	
	// Important:
	// Build the resolver.
	rWrapper, err := newCCResolverWrapper(cc)
	if err != nil {
		return nil, fmt.Errorf("failed to build resolver: %v", err)
	}
}
```

首先， Client 需要透過 **Name Resolver** 解析 Dial 中的 target string 來取得 Server 正確的 IP Address，以利後續建立 connection。Name Resolver 是 gRPC 實現 load-balancing 的主角之一，例如使用 DNS Resolver，就可以透過 `conn, err := grpc.Dial("www.example.com:8888")` 這種 Domain name 的方式來發送 gRPC。

在沒有指定 `resolverBuilder` 情況下， gRPC 會使用預設 `passthrough resolver` 去解析名稱(passthrough 其實就是很單純地把 target 再返回來，因此僅適用於簡單應用或是測試)。

```go
func newCCResolverWrapper(cc *ClientConn) (*ccResolverWrapper, error) {
  ccr.resolver, err = rb.Build(cc.parsedTarget, ccr, resolver.BuildOption{DisableServiceConfig: cc.dopts.disableServiceConfig})
}

// Default use passthrough Builder.
func (*passthroughBuilder) Build(target resolver.Target, cc resolver.ClientConn, opts resolver.BuildOption) (resolver.Resolver, error) {
  r := &passthroughResolver{
		target: target,
		cc:     cc,
	}
	r.start()
}

func (r *passthroughResolver) start() {
  // r.cc is ccResolverWrapper
	r.cc.UpdateState(resolver.State{Addresses: []resolver.Address{{Addr: r.target.Endpoint}}})
}

func (ccr *ccResolverWrapper) UpdateState(s resolver.State) {
  //  ccr.cc is ClientConn
  ccr.cc.updateResolverState(s)
}
```

當 Name Resolver 解析完名稱之後，會透過 watcher 通知 `Balancer` 最新的 Addresses。

**Balancer** 同樣是也是 gRPC load-balacing 要角，它最主要負責 Handle connection 和 addresses 的變化，以及在後續 transport 階段決定該選擇哪個 connection。

> **Balancer** takes input from gRPC, manages SubConns, and collects and aggregates the connectivity states. It also generates and updates the Picker used by gRPC to pick SubConns for RPCs.

> **SubConn** represents a gRPC sub connection. Each sub connection contains a list of addresses. gRPC will try to connect to them (in sequence), and stop trying the remainder once one connection is successful.

![gRPC-Dial-1.png]({{ site.url }}/assets/images/gRPC-Dial-1.png)

```go
func (cc *ClientConn) updateResolverState(s resolver.State) error {
  if cc.dopts.balancerBuilder == nil {
    if isGRPCLB {
			newBalancerName = grpclbName
		} else if cc.sc != nil && cc.sc.LB != nil {
			newBalancerName = *cc.sc.LB
		} else {
		  // Default use pick first balancer(pick_first.go)
			newBalancerName = PickFirstBalancerName
		}
		cc.switchBalancer(newBalancerName)
  }
  cc.balancerWrapper.updateResolverState(s)
}

// Use resolverUpdateCh to pass resolved Addresses to Balancer
func (ccb *ccBalancerWrapper) updateResolverState(s resolver.State) {
  ccb.resolverUpdateCh <- &s
}

func (cc *ClientConn) switchBalancer(name string) {
  cc.balancerWrapper = newCCBalancerWrapper(cc, builder, cc.balancerBuildOpts)
}

func newCCBalancerWrapper(cc *ClientConn, b balancer.Builder, bopts balancer.BuildOptions) *ccBalancerWrapper {
  ccb := &ccBalancerWrapper{
		cc:               cc,
		stateChangeQueue: newSCStateUpdateBuffer(),
		resolverUpdateCh: make(chan *resolver.State, 1),
		done:             make(chan struct{}),
		subConns:         make(map[*acBalancerWrapper]struct{}),
	}
	go ccb.watcher() // Create a watcher for name resolved events.
	ccb.balancer = b.Build(ccb, bopts)
	return ccb
}
```

而 `Balancer Wrapper` 則是扮演 clientConn 和 balancer 的中間角色，它會開啟一個 `watcher` 來監控 update event。

```go
func (ccb *ccBalancerWrapper) watcher() {
  for {
		select {
		case s := <-ccb.resolverUpdateCh:
			select {
			case <-ccb.done:
				ccb.balancer.Close()
				return
			default:
			}
			if ub, ok := ccb.balancer.(balancer.V2Balancer); ok {
				ub.UpdateResolverState(*s)
			} else {
				ccb.balancer.HandleResolvedAddrs(s.Addresses, nil)
			}
		}
	}
}
```

這邊以 pick first balancer 為例，觀察 Balancer 處理 resolved addresses 行為。

```go
type pickfirstBalancer struct {
	cc balancer.ClientConn // 這邊是指 balancer_conn_wrapper
	sc balancer.SubConn // 維護一條 subConn
}

func (b *pickfirstBalancer) HandleResolvedAddrs(addrs []resolver.Address, err error) {
	if err != nil {
		grpclog.Infof("pickfirstBalancer: HandleResolvedAddrs called with error %v", err)
		return
	}
	if b.sc == nil {
		b.sc, err = b.cc.NewSubConn(addrs, balancer.NewSubConnOptions{})
		if err != nil {
			grpclog.Errorf("pickfirstBalancer: failed to NewSubConn: %v", err)
			return
		}
		b.cc.UpdateBalancerState(connectivity.Idle, &picker{sc: b.sc})
		b.sc.Connect() // Connect to server (b.sc is balancer_conn_wrapper)
	} else {
		b.sc.UpdateAddresses(addrs)
		b.sc.Connect()
	}
}
```

上面可以看到如果沒有 subConn ，則會透過 `balancer_conn_wrapper` 新建一條 SubConn，並且觸發 Connnect。

```go
func (ac *addrConn) connect() error {
  ac.updateConnectivityState(connectivity.Connecting)
	ac.mu.Unlock()

	// Start a goroutine connecting to the server asynchronously.
	go ac.resetTransport()
}
```

![gRPC-Dial-2.png]({{ site.url }}/assets/images/gRPC-Dial-2.png)

```go
func (ac *addrConn) resetTransport() {
  // Reconnect forever
	for i := 0; ; i++ {
		if i > 0 {
			ac.cc.resolveNow(resolver.ResolveNowOption{})
		}

		ac.mu.Lock()
		if ac.state == connectivity.Shutdown {
			ac.mu.Unlock()
			return
		}

		newTr, addr, reconnect, err := ac.tryAllAddrs(addrs, connectDeadline)
		if err != nil {
			ac.mu.Lock()
			if ac.state == connectivity.Shutdown {
				ac.mu.Unlock()
				return
			}
			ac.updateConnectivityState(connectivity.TransientFailure)

			// Backoff.
			b := ac.resetBackoff
			ac.mu.Unlock()

      // wait and reconnect
		  timer := time.NewTimer(backoffFor)
			select {
			case <-timer.C:
				ac.mu.Lock()
				ac.backoffIdx++
				ac.mu.Unlock()
			case <-b:
				timer.Stop()
			case <-ac.ctx.Done():
				timer.Stop()
				return
			}
			continue
		}

		ac.mu.Lock()
		if ac.state == connectivity.Shutdown {
			newTr.Close()
			ac.mu.Unlock()
			return
		}
		ac.curAddr = addr
		ac.transport = newTr
		ac.backoffIdx = 0

    // A connection with Ready state can be picked
    if !healthcheckManagingState {
			ac.updateConnectivityState(connectivity.Ready)
		}
		ac.mu.Unlock()
		
		// Block until the created transport is down. And when this happens,
		// we restart from the top of the addr list.
		<-reconnect.Done()
		hcancel()
}
```

`resetTransport` 是一個無限循環的迴圈，意味著如果 Server 端異常導致 disconnect 時，client 端會重新嘗試連線，直到連線成功或是 `connectivity.Shutdown` 為止。

如果在一開始的 `grpc.Dial` 額外設置 `grpc.WithBlock`，則會等到確認 `connectivity.Ready` 後才會返回。不然在預設狀態下是 non-blocking ，讓 Client 在等待連線成功之餘可以做更多事情。

## References
1. https://github.com/grpc/grpc/blob/master/doc/load-balancing.md